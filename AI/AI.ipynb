{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cuml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# from sklearn.svm import SVC\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SGDClassifier\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcuml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cuml'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "# from cuml import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./Data.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# df = df[['networkExp', 'bedwars_level', 'Experience', 'beds_broken_bedwars', 'beds_lost_bedwars', 'coins', 'deaths_bedwars', 'diamond_resources_collected_bedwars', 'emerald_resources_collected_bedwars', 'final_deaths_bedwars', 'final_kills_bedwars', 'games_played_bedwars', 'games_played_bedwars_1', 'kills_bedwars', 'losses_bedwars', 'void_final_deaths_bedwars',  'wins_bedwars','fkdr','wlr','bblr','fk_lev','bb_lev','kill_lev', \"Cheat\"]]\n",
    "df = df[['karma', 'bedwars_level',\n",
    "        # 'bedwars_loot_box',\n",
    "        # 'all_timeBEDWARS__defensive', 'BEDWARS__offensive',\n",
    "        # 'BEDWARS__support', 'Bedwars_openedChests',\n",
    "        # 'beds_broken_bedwars', 'beds_lost_bedwars',\n",
    "        # 'deaths_bedwars', 'diamond_resources_collected_bedwars',\n",
    "        # 'emerald_resources_collected_bedwars', 'fall_deaths_bedwars',\n",
    "        # 'final_deaths_bedwars', 'final_kills_bedwars',\n",
    "        # 'games_played_bedwars', 'games_played_bedwars_1', 'kills_bedwars',\n",
    "        # 'losses_bedwars', 'void_deaths_bedwars', 'void_final_deaths_bedwars',\n",
    "        'void_final_kills_bedwars', 'void_kills_bedwars', 'wins_bedwars', \"Cheat\"\n",
    "        ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            karma  bedwars_level  void_final_kills_bedwars  \\\n",
      "0     9460.116732      16.212711                175.097276   \n",
      "1     1319.648094     410.557185               2199.413490   \n",
      "2     9979.133185       0.907253                  4.017834   \n",
      "3     9589.041096      21.074816                158.061117   \n",
      "4        0.000000     596.026490                761.589404   \n",
      "...           ...            ...                       ...   \n",
      "4673  6995.478833       2.055076                524.044390   \n",
      "4674   449.640288      17.985612                683.453237   \n",
      "4675  3773.584906      15.723270                707.547170   \n",
      "4676  2807.646356      11.947431               1206.690562   \n",
      "4677  1785.714286      59.523810                833.333333   \n",
      "\n",
      "      void_kills_bedwars  wins_bedwars  Cheat  \n",
      "0             322.632944     25.940337      0  \n",
      "1            5718.475073    351.906158      0  \n",
      "2              13.219970      2.721759      0  \n",
      "3             168.598525     63.224447      0  \n",
      "4            7814.569536    827.814570      0  \n",
      "...                  ...           ...    ...  \n",
      "4673         1944.101932    534.319770      1  \n",
      "4674         6420.863309   2428.057554      1  \n",
      "4675         4261.006289   1242.138365      1  \n",
      "4676         4086.021505   1887.694146      1  \n",
      "4677         3988.095238   3333.333333      1  \n",
      "\n",
      "[4678 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Cheatカラムを削除して、各レコードの合計を計算する\n",
    "row_sums = df.drop('Cheat', axis=1).sum(axis=1)\n",
    "\n",
    "# 分母が0になる可能性がある行のインデックスを取得\n",
    "zero_division_indices = row_sums[row_sums == 0].index\n",
    "\n",
    "# 10,000をその行の合計値で割った値を計算して保持する\n",
    "divisors = pd.Series(0, index=df.index)  # 全ての行を0で初期化\n",
    "divisors[zero_division_indices] = 0  # 分母が0になる行のみ0に設定\n",
    "divisors[~divisors.index.isin(zero_division_indices)] = 10000 / row_sums[~row_sums.index.isin(zero_division_indices)]\n",
    "\n",
    "# それぞれの行に対して計算した値を掛ける\n",
    "df_scaled = df.drop('Cheat', axis=1).mul(divisors, axis=0)\n",
    "\n",
    "# データフレームにCheatカラムを追加する\n",
    "df_scaled['Cheat'] = df['Cheat']\n",
    "print(df_scaled)\n",
    "\n",
    "# 新しいデータベースとして保存する\n",
    "# df_scaled.to_csv('ScaledData.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing indices: [42, 98, 104, 113, 117, 124, 130, 138, 154, 159, 162, 170, 173, 192, 218, 220, 223, 246, 269, 294, 302, 348, 379, 383, 398, 405, 415, 483, 489, 530, 538, 541, 543, 584, 599, 636, 727, 776, 812, 825, 847, 878, 883, 918, 991, 1011, 1118, 1185, 1220, 1223, 1267, 1275, 1298, 1351, 1357, 1362, 1376, 1385, 1389, 1412, 1423, 1450, 1487, 1490, 1509, 1511, 1513, 1515, 1522, 1530, 1556, 1563, 1567, 1571, 1632, 1657, 1698, 1705, 1710, 1742, 1761, 1767, 1828, 1831, 1922, 1955, 2065, 2066, 2078, 2111, 2134, 2139, 2146, 2214, 2216, 2284, 2299, 2331, 2340, 2345, 2346, 2351, 2369, 2382, 2383, 2385, 2404, 2436, 2445, 2448, 2453, 2460, 2464, 2471, 2481, 2483, 2490, 2498, 2500, 2519, 2534, 2542, 2554, 2558, 2565, 2566, 2573, 2574, 2575, 2584, 2612, 2623, 2626, 2627, 2645, 2657, 2664, 2673, 2678, 2702, 2707, 2719, 2727, 2734, 2743, 2747, 2762, 2776, 2777, 2784, 2802, 2804, 2810, 2812, 2817, 2818, 2846, 2848, 2877, 2878, 2895, 2905, 2906, 2923, 2943, 2970, 2976, 2992, 2998, 3001, 3009, 3010, 3015, 3016, 3021, 3029, 3034, 3052, 3056, 3067, 3072, 3126, 3129, 3140, 3156, 3161, 3166, 3200, 3210, 3220, 3222, 3225, 3231, 3276, 3295, 3305, 3307, 3311, 3314, 3328, 3332, 3335, 3351, 3357, 3362, 3374, 3385, 3392, 3407, 3417, 3425, 3428, 3443, 3457, 3471, 3473, 3483, 3491, 3501, 3503, 3506, 3508, 3517, 3524, 3529, 3546, 3569, 3571, 3572, 3573, 3580, 3581, 3583, 3588, 3605, 3607, 3621, 3630, 3657, 3660, 3664, 3682, 3683, 3705, 3711, 3712, 3735, 3742, 3759, 3767, 3795, 3827, 3840, 3842, 3859, 3869, 3894, 3902, 3910, 3915, 3926, 3930, 3948, 3961, 3962, 3964, 3966, 3976, 3978, 4001, 4004, 4007, 4008, 4015, 4028, 4031, 4056, 4065, 4100, 4105, 4130, 4159, 4168, 4172, 4175, 4178, 4217, 4218, 4224, 4231, 4237, 4250, 4269, 4279, 4281, 4284, 4285, 4302, 4312, 4322, 4332, 4339, 4358, 4391, 4410, 4444, 4453, 4473, 4475, 4498, 4508, 4524, 4525, 4526, 4557, 4558, 4613, 4614, 4620, 4621, 4634, 4669]\n",
      "Accuracy: 0.48717948717948717\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "Confusion Matrix:\n",
      "[[228   0]\n",
      " [240   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# df = pd.get_dummies(df)\n",
    "# print(\"dummies\")\n",
    "\n",
    "X = df_scaled.drop('Cheat', axis=1)\n",
    "y = df_scaled['Cheat']\n",
    "df_scaled\n",
    "\n",
    "# データの標準化\n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(X)\n",
    "\n",
    "# 訓練データとテストデータに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "\n",
    "# 不足しているデータのインデックスを特定\n",
    "underrepresented_indices = df_scaled[df_scaled['bedwars_level'] <= 20].index\n",
    "\n",
    "missing_indices = [index for index in underrepresented_indices if index not in X_train.index]\n",
    "print(\"Missing indices:\", missing_indices)\n",
    "\n",
    "# Filter out the missing indices\n",
    "underrepresented_indices = [index for index in underrepresented_indices if index in X_train.index]\n",
    "X_underrepresented = X_train.loc[underrepresented_indices]\n",
    "y_underrepresented = y_train.loc[underrepresented_indices]\n",
    "\n",
    "\n",
    "# 不足しているデータだけを抽出\n",
    "X_underrepresented = X_train.loc[underrepresented_indices]\n",
    "y_underrepresented = y_train.loc[underrepresented_indices]\n",
    "\n",
    "# SMOTEでオーバーサンプリングを実行\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_underrepresented, y_underrepresented)\n",
    "\n",
    "# オーバーサンプリング後のデータを元のデータに結合\n",
    "X_train_resampled = pd.concat([X_train, X_resampled])\n",
    "y_train_resampled = pd.concat([y_train, y_resampled])\n",
    "\n",
    "# モデルの作成と訓練（オーバーサンプリング後のデータを使用）\n",
    "# model = RandomForestClassifier(n_estimators=2000, random_state=42, class_weight={0: 1, 1: 10})\n",
    "# model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "# model = LogisticRegression(random_state=42)\n",
    "model = SVC(kernel='linear', probability=True, random_state=42)\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "\n",
    "\n",
    "# テストデータを使って予測を行う\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "# # モデルの作成\n",
    "# model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# # モデルの訓練\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # テストデータを使って予測を行う\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# 正解率を計算する\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# 適合率を計算する\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(f'Precision: {precision}')\n",
    "\n",
    "# 再現率を計算する\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(f'Recall: {recall}')\n",
    "\n",
    "# F1スコアを計算する\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f'F1 Score: {f1}')\n",
    "\n",
    "# 混同行列を表示する\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "# モデルと標準化器の保存\n",
    "# joblib.dump(model, './models/Cheater.pkl')\n",
    "# joblib.dump(scaler, './models/scaler.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Cheater.pkl']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, '../Cheater.pkl')\n",
    "# joblib.dump(scaler, './models/scaler.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# snipe垢の条件\n",
    "\n",
    "fkdr, wlr, bblrは低い\n",
    "\n",
    "star n.levは30以下\n",
    "\n",
    "sniper%が高い　（ターゲットが見つかるまで何回もｒｑするから）\n",
    "\n",
    "fk/lev bb/lev kill/lev が高い　fk/levは7.5 bb/levは6.5 kill/levは20以上で高い"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cheatersデータ - snipe垢 = not sniper cheaters\n",
    "snipe垢の条件外のcheaterを予測する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cubelifyのsniper%\n",
    "cubelifyのsniper%を取得して、学習データとする"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
